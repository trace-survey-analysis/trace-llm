# TRACE LLM - Gemini RAG Service

![Google](https://img.shields.io/badge/Google-4285F4.svg?style=for-the-badge&logo=google&logoColor=white)
![Gemini](https://img.shields.io/badge/Google_Gemini-8E75B2?style=for-the-badge&logo=google&logoColor=white)
![AI](https://img.shields.io/badge/Artificial_Intelligence-FF6F00.svg?style=for-the-badge&logo=openai&logoColor=white)
![LLM](https://img.shields.io/badge/LLM-0078D4.svg?style=for-the-badge&logo=openai&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB.svg?style=for-the-badge&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688.svg?style=for-the-badge&logo=fastapi&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-4169E1.svg?style=for-the-badge&logo=postgresql&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED.svg?style=for-the-badge&logo=docker&logoColor=white)
![Jenkins](https://img.shields.io/badge/Jenkins-D24939.svg?style=for-the-badge&logo=jenkins&logoColor=white)
![Semantic Release](https://img.shields.io/badge/Semantic_Release-494949.svg?style=for-the-badge&logo=semantic-release&logoColor=white)

A Retrieval-Augmented Generation (RAG) service that leverages Google's Gemini AI to provide natural language insights and answers from TRACE survey data.

## Overview

The TRACE LLM service enables natural language querying of TRACE survey data, allowing users to ask questions about courses, instructors, and student feedback in plain English. The service uses a RAG architecture to:

1. Process natural language questions about TRACE surveys
2. Detect entities (instructors, courses) mentioned in the question
3. Retrieve relevant contexts from a vector database
4. Generate comprehensive, accurate answers using Google's Gemini AI

This service is part of the trace-survey-analysis platform, connecting to the vector embeddings generated by the [embedding-service](https://github.com/cyse7125-sp25-team03/embedding-service.git) and building upon the data ingested by the [trace-consumer](https://github.com/cyse7125-sp25-team03/trace-consumer.git).

## Architecture

### Components

- **FastAPI Application**: Exposes REST API endpoints for querying the system
- **Query Processor**: Analyzes questions to detect entities and query intent
- **Semantic Search**: Retrieves relevant contexts using vector similarity search
- **LLM Integration**: Communicates with Google's Gemini AI for answer generation
- **Prompt Engineering**: Customizes prompts based on query type for optimal responses

### Data Flow

1. User submits a natural language query through the API
2. System analyzes the query to detect mentions of specific instructors or courses
3. Query is converted to a vector embedding for similarity search
4. Vector database is searched to find relevant contexts (comments, ratings, instructor info, course info)
5. Relevant contexts are assembled and sent to Gemini with a custom prompt
6. Gemini generates a natural language response that directly answers the question
7. Response and source information are returned to the user

## Features

- **Natural Language Understanding**: Ask questions in plain English about courses and instructors
- **Entity Recognition**: Automatically detects mentions of instructors and courses
- **Specialized Responses**: Tailored answers for different query types (instructor-focused, course-focused, general)
- **Source Attribution**: Responses include metadata about the information sources
- **Semantic Search**: Finds information based on meaning, not just keywords
- **Configurable Parameters**: Adjust similarity thresholds, result counts, and other parameters

## API Endpoints

- **GET /health**: Health check endpoint
- **POST /query**: Main query endpoint
  - Request body:
    ```json
    {
      "question": "How is Professor Smith's teaching style?",
      "top_k": 10,
      "similarity_threshold": 0.6
    }
    ```
  - Response:
    ```json
    {
      "answer": "Based on the TRACE surveys, Professor Smith is known for...",
      "sources": [
        {
          "id": 123,
          "type": "comment",
          "similarity": 0.92,
          "course_id": "CS101",
          "course_name": "Introduction to Computer Science",
          "semester": "Fall",
          "year": 2023,
          "question": "What did you like most about this instructor?",
          "category": "Instructor Feedback"
        },
        ...
      ]
    }
    ```

## Requirements

- Python 3.8+
- PostgreSQL with pgvector extension
- Google Cloud API key with Gemini access
- Vector embeddings for TRACE data (generated by [embedding-service](https://github.com/cyse7125-sp25-team03/embedding-service.git))

## Configuration

The service is configured through environment variables:

### Database Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `DB_HOST` | PostgreSQL host | `localhost` |
| `DB_PORT` | PostgreSQL port | `5432` |
| `DB_NAME` | Database name | `trace` |
| `DB_USER` | Database username | `postgres` |
| `DB_PASSWORD` | Database password | `""` |

### Embedding Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `EMBEDDING_SCHEMA` | Schema for vector tables | `vectors` |
| `SOURCE_SCHEMA` | Schema for source tables | `trace` |
| `EMBEDDING_MODEL` | Model for generating embeddings | `all-MiniLM-L6-v2` |
| `EMBEDDING_DIM` | Embedding dimension | `384` |

### Gemini Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI_API_KEY` | Google Gemini API key | `None` (Required) |
| `GEMINI_MODEL` | Gemini model to use | `gemini-2.0-flash` |
| `GEMINI_TEMPERATURE` | Temperature for generation | `0.2` |
| `GEMINI_TOP_P` | Top-p sampling parameter | `0.8` |
| `GEMINI_TOP_K` | Top-k sampling parameter | `40` |
| `GEMINI_MAX_OUTPUT_TOKENS` | Maximum output length | `1024` |

### RAG Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `RAG_DEFAULT_TOP_K` | Default number of contexts to retrieve | `10` |
| `RAG_DEFAULT_SIMILARITY_THRESHOLD` | Minimum similarity score | `0.6` |

### Service Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `SERVICE_PORT` | HTTP server port | `8000` |
| `SERVICE_HOST` | HTTP server host | `0.0.0.0` |

## Installation

### Local Development

1. Clone the repository:
```bash
git clone https://github.com/cyse7125-sp25-team03/trace-llm.git
cd trace-llm
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file with your configuration:
```
DB_HOST=localhost
DB_PORT=5432
DB_NAME=trace
DB_USER=postgres
DB_PASSWORD=yourpassword

GEMINI_API_KEY=your_gemini_api_key
```

5. Run the service:
```bash
python -m src.main
```

### Docker Deployment

1. Build the Docker image:
```bash
docker build -t trace-llm .
```

2. Run the container:
```bash
docker run -p 8000:8000 \
  -e DB_HOST=your_db_host \
  -e DB_PASSWORD=your_db_password \
  -e GEMINI_API_KEY=your_gemini_api_key \
  trace-llm
```

### Kubernetes Deployment with Helm

Deploy to Kubernetes using the Helm chart from the [helm-charts repository](https://github.com/cyse7125-sp25-team03/helm-charts.git):

```bash
# Clone the helm-charts repository
git clone https://github.com/cyse7125-sp25-team03/helm-charts.git
cd helm-charts

# Install the trace-llm chart
helm install trace-llm ./trace-llm/ -n trace-llm
```

## Example Queries

Here are some example queries you can try:

- "How is Professor Johnson's teaching style?"
- "What do students think about CS5010?"
- "What are the strengths and weaknesses of the Computer Science department?"
- "Which instructor received the highest ratings in Fall 2023?"
- "What feedback did students give about the workload in Data Structures?"

## Debugging

To debug database connectivity issues, you can run:

```bash
python -m src.db_debug
```

This will:
- Test the database connection
- Verify schemas exist
- Check embedding tables
- Count available records

## CI/CD

This project uses Jenkins for continuous integration and Semantic Release for versioning:

- When a pull request is successfully merged, a Docker image is built
- The Semantic Versioning bot creates a release on GitHub with a tag
- The tagged release is used for the Docker image, which is then pushed to Docker Hub

## Related Repositories

- [api-server](https://github.com/cyse7125-sp25-team03/api-server.git) - Main API service
- [trace-consumer](https://github.com/cyse7125-sp25-team03/trace-consumer.git) - Processes trace surveys
- [embedding-service](https://github.com/cyse7125-sp25-team03/embedding-service.git) - Generates vector embeddings
- [db-trace-processor](https://github.com/cyse7125-sp25-team03/db-trace-processor.git) - Database migrations
- [helm-charts](https://github.com/cyse7125-sp25-team03/helm-charts.git) - Helm charts for deployment

## License

This project is licensed under the GNU General Public License v3.0. See the [LICENSE](LICENSE) file for details.